# generate_eps.py
import os, sys, json, time, random
import requests
import pandas as pd
from bs4 import BeautifulSoup
from io import StringIO
from pathlib import Path

STOCK_IDS = ["2330", "2317", "2303"]
OUTFILE = "eps_cache.json"
DEBUG_DIR = Path("debug")
DEBUG_DIR.mkdir(exist_ok=True)

BASE = "https://goodinfo.tw"
PATH = "/tw/StockBzPerformance.asp"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/124.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
    "Referer": f"{BASE}{PATH}",
}
# é€™å€‹ç«™å¸¸çœ‹ Referer / UAï¼ŒCookie ä¸ä¸€å®šè¦ï¼Œä½†åŠ ä¸Šå¯æé«˜æˆåŠŸç‡
COOKIES = {
    "IS_TOUCH_DEVICE": "F",
    "SCREEN_SIZE": "WIDTH=2048&HEIGHT=1280",
}

def fetch_html(session: requests.Session, stock_id: str) -> str | None:
    url = f"{BASE}{PATH}?STOCK_ID={stock_id}"
    for i in range(3):
        try:
            r = session.get(url, headers=HEADERS, cookies=COOKIES, timeout=15)
            r.encoding = "utf-8"
            if r.status_code == 200 and "html" in r.headers.get("content-type", ""):
                return r.text
        except Exception:
            pass
        time.sleep(1 + random.random())
    return None

def parse_table(html: str, stock_id: str) -> list[dict]:
    soup = BeautifulSoup(html, "lxml")

    box = soup.select_one("#txtFinDetailData") or soup.select_one("table")
    if not box:
        return []

    dfs = pd.read_html(StringIO(box.prettify()))
    if not dfs:
        return []
    df = dfs[0]
    df.columns = [str(c).strip() for c in df.columns.map(str)]

    # æ¬„ä½åµæ¸¬
    def find_col(cols, needles):
        for c in cols:
            cc = str(c)
            if any(n in cc for n in needles):
                return c
        return None

    year_col     = find_col(df.columns, ["å¹´åº¦", "å¹´/å­£", "å¹´å­£", "å¹´å­£åˆ¥"])
    eps_col      = find_col(df.columns, ["EPS"])
    roe_col      = find_col(df.columns, ["ROE"])
    priceavg_col = find_col(df.columns, ["å¹³å‡"])

    rows = []
    for _, row in df.iterrows():
        # âœ… ä¸åšæ­£è¦åŒ–ï¼Œç›´æ¥ç”¨åŸå§‹æ–‡å­—
        year = str(row.get(year_col, "")).strip() if year_col else ""
        if not year:
            continue

        rows.append({
            "year": year,                          # â† åŸæ¨£ä¿ç•™ï¼ˆå¯èƒ½æ˜¯ 25Q1ã€2024 å¹´ä¼°â€¦ï¼‰
            "eps": "" if not eps_col else str(row.get(eps_col, "")).strip(),
            "ROE": "" if not roe_col else str(row.get(roe_col, "")).strip(),
            "priceavg": "" if not priceavg_col else str(row.get(priceavg_col, "")).strip(),
        })

    # å¦‚æœä½ ä¸è¦æ’åºï¼Œä¹Ÿå¯ä»¥æ‹¿æ‰é€™è¡Œ
    # rows.sort(key=lambda r: r["year"], reverse=True)

    return rows


def main():
    out = {}
    total = 0

    with requests.Session() as s:
        for sid in STOCK_IDS:
            html = fetch_html(s, sid)
            # å­˜ raw HTML ä¾¿æ–¼ä½ äº‹å¾Œä¸‹è¼‰æª¢æŸ¥
            (DEBUG_DIR / f"page_{sid}.html").write_text(html or "", encoding="utf-8")

            if not html:
                print(f"âš ï¸  {sid} æŠ“ä¸åˆ° HTML")
                out[sid] = []
                continue

            rows = parse_table(html, sid)
            # ä¹ŸæŠŠæ¬„ä½è³‡è¨Šå­˜ä¸‹ä¾†ï¼Œå¹«ä½ æŸ¥æ¬„ä½åæ˜¯å¦è®Šäº†
            if rows:
                # çµ¦ä½ ä¸€ä»½ table çš„å‰å¹¾åˆ— CSV
                try:
                    soup = BeautifulSoup(html, "lxml")
                    box = soup.select_one("#txtFinDetailData") or soup.select_one("table")
                    dfs = pd.read_html(StringIO(box.prettify()))
                    dfs[0].head(10).to_csv(DEBUG_DIR / f"table_head_{sid}.csv", index=False)
                except Exception:
                    pass

            out[sid] = rows
            total += len(rows)
            print(f"âœ… {sid}ï¼š{len(rows)} ç­†")

    # å…ˆå°‡çµæœå¯«åˆ°æª”æ¡ˆï¼ˆå°±ç®— 0 ç­†ï¼Œä¹Ÿè®“ä½ åœ¨ artifact ä¸‹è¼‰åˆ°ï¼‰ï¼Œ
    # ä½†ã€Œ0 ç­†ã€æ™‚æˆ‘å€‘æœƒç”¨é 0 çš„ exit code è®“ workflow å¤±æ•—ï¼Œé¿å…æŠŠç©ºè³‡æ–™ push ä¸Š repoã€‚
    with open(OUTFILE, "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“ å¯«å…¥ {OUTFILE} å®Œæˆï¼ˆç¸½å…± {total} ç­†ï¼‰")

    # è‹¥å…¨éƒ¨éƒ½ 0ï¼Œè®“ workflow å¤±æ•—ï¼Œä¸¦å¼•å°ä½ ä¸‹è¼‰ debug æª”
    if total == 0:
        print("âŒ å…¨éƒ¨è‚¡ç¥¨çš†ç‚º 0 ç­†ï¼Œè«‹åˆ° Actions ä¸‹è¼‰ debug artifact æª¢æŸ¥ page_*.html / table_head_*.csv")
        sys.exit(2)

if __name__ == "__main__":
    main()



