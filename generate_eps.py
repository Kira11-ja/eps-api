# generate_eps.py
import os, sys, json, time, random
import requests
import pandas as pd
from bs4 import BeautifulSoup
from io import StringIO
from pathlib import Path

STOCK_IDS = ["2330", "2317", "2303"]
OUTFILE = "eps_cache.json"
DEBUG_DIR = Path("debug")
DEBUG_DIR.mkdir(exist_ok=True)

BASE = "https://goodinfo.tw"
PATH = "/tw/StockBzPerformance.asp"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/124.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
    "Referer": f"{BASE}{PATH}",
}
# é€™å€‹ç«™å¸¸çœ‹ Referer / UAï¼ŒCookie ä¸ä¸€å®šè¦ï¼Œä½†åŠ ä¸Šå¯æé«˜æˆåŠŸç‡
COOKIES = {
    "IS_TOUCH_DEVICE": "F",
    "SCREEN_SIZE": "WIDTH=2048&HEIGHT=1280",
}

def fetch_html(session: requests.Session, stock_id: str) -> str | None:
    url = f"{BASE}{PATH}?STOCK_ID={stock_id}"
    for i in range(3):
        try:
            r = session.get(url, headers=HEADERS, cookies=COOKIES, timeout=15)
            r.encoding = "utf-8"
            if r.status_code == 200 and "html" in r.headers.get("content-type", ""):
                return r.text
        except Exception:
            pass
        time.sleep(1 + random.random())
    return None

def parse_table(html: str, stock_id: str) -> list[dict]:
    soup = BeautifulSoup(html, "lxml")

    # å¸¸è¦‹å®¹å™¨ï¼š#txtFinDetailDataï¼ˆæœ‰æ™‚å€™æœƒè®Šï¼‰ï¼Œå‚™æ´æŠ“ç¬¬ä¸€å€‹åŒ…å«ã€Œå¹´åº¦ã€å­—æ¨£çš„è¡¨
    data_box = soup.select_one("#txtFinDetailData")
    if not data_box:
        # å‚™æ´ï¼šæŠ“ç¬¬ä¸€å€‹ tableï¼Œè®“ä½ è‡³å°‘çœ‹åˆ°æ¬„ä½ä»¥ä¾¿ debug
        tbl = soup.select_one("table")
        if not tbl:
            return []
        data_box = tbl

    try:
        dfs = pd.read_html(StringIO(data_box.prettify()))
        if not dfs:
            return []
        df = dfs[0]
        df.columns = df.columns.map(str)
    except Exception:
        return []

    # å¯èƒ½çš„æ¬„ä½åç¨±ï¼ˆç«™æ–¹æœƒè®Šå‹•ï¼‰
    YEAR_KEYS = ["å¹´åº¦", "å¹´/å­£", "å¹´å­£", "å¹´å­£åˆ¥"]
    EPS_KEYS  = ["EPS(å…ƒ)", "ç¨…å¾ŒEPS(å…ƒ)", "ç¨…å¾ŒEPS"]
    YOY_KEYS  = ["å¹´å¢(å…ƒ)", "å¹´å¢ç‡(å…ƒ)", "å¹´å¢ç‡"]
    PE_KEYS   = ["æœ¬ç›Šæ¯”", "æœ¬ç›Šæ¯”(å€)"]

    def first_col(row, keys):
        for k in keys:
            if k in row:
                return str(row[k]).strip()
        return ""

    rows = []
    for _, row in df.iterrows():
        year = first_col(row, YEAR_KEYS)
        if not year or year.lower() in ("nan", "none"):
            continue
        rows.append({
            "year": year,
            "eps": first_col(row, EPS_KEYS),
            "yoy": first_col(row, YOY_KEYS),
            "pe":  first_col(row, PE_KEYS),
        })
    return rows

def main():
    out = {}
    total = 0

    with requests.Session() as s:
        for sid in STOCK_IDS:
            html = fetch_html(s, sid)
            # å­˜ raw HTML ä¾¿æ–¼ä½ äº‹å¾Œä¸‹è¼‰æª¢æŸ¥
            (DEBUG_DIR / f"page_{sid}.html").write_text(html or "", encoding="utf-8")

            if not html:
                print(f"âš ï¸  {sid} æŠ“ä¸åˆ° HTML")
                out[sid] = []
                continue

            rows = parse_table(html, sid)
            # ä¹ŸæŠŠæ¬„ä½è³‡è¨Šå­˜ä¸‹ä¾†ï¼Œå¹«ä½ æŸ¥æ¬„ä½åæ˜¯å¦è®Šäº†
            if rows:
                # çµ¦ä½ ä¸€ä»½ table çš„å‰å¹¾åˆ— CSV
                try:
                    soup = BeautifulSoup(html, "lxml")
                    box = soup.select_one("#txtFinDetailData") or soup.select_one("table")
                    dfs = pd.read_html(StringIO(box.prettify()))
                    dfs[0].head(10).to_csv(DEBUG_DIR / f"table_head_{sid}.csv", index=False)
                except Exception:
                    pass

            out[sid] = rows
            total += len(rows)
            print(f"âœ… {sid}ï¼š{len(rows)} ç­†")

    # å…ˆå°‡çµæœå¯«åˆ°æª”æ¡ˆï¼ˆå°±ç®— 0 ç­†ï¼Œä¹Ÿè®“ä½ åœ¨ artifact ä¸‹è¼‰åˆ°ï¼‰ï¼Œ
    # ä½†ã€Œ0 ç­†ã€æ™‚æˆ‘å€‘æœƒç”¨é 0 çš„ exit code è®“ workflow å¤±æ•—ï¼Œé¿å…æŠŠç©ºè³‡æ–™ push ä¸Š repoã€‚
    with open(OUTFILE, "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“ å¯«å…¥ {OUTFILE} å®Œæˆï¼ˆç¸½å…± {total} ç­†ï¼‰")

    # è‹¥å…¨éƒ¨éƒ½ 0ï¼Œè®“ workflow å¤±æ•—ï¼Œä¸¦å¼•å°ä½ ä¸‹è¼‰ debug æª”
    if total == 0:
        print("âŒ å…¨éƒ¨è‚¡ç¥¨çš†ç‚º 0 ç­†ï¼Œè«‹åˆ° Actions ä¸‹è¼‰ debug artifact æª¢æŸ¥ page_*.html / table_head_*.csv")
        sys.exit(2)

if __name__ == "__main__":
    main()
